---
title: "What's So Bad About Being Rational Anyway?"
date: "2024-10-03"
excerpt: "Why I harbour a visceral distaste for the rationalist movement and its offspring Effective Altruism—despite them being right about many things. The rot at the core of an ideology that thinks you can do math to justify anything."
shortTitle: "What's Wrong With Rationalists"
---

If you've spent a lot of time in tech-adjacent or pseudointellectual corners of the internet you have almost certainly run into rationalists or at least their ideology. Tech-bro philosopher-would-be-kings are currently having their moment in the sun, so I wanted to articulate why I harbour a visceral distaste for these folks. The AI circles are especially full of adherents of this ideology and its more severe spin-off "Effective Altruism" (EA). Frustratingly, from the perspective of a hater like myself, "Rationalists" are right about many of the things they espouse. Even the EA folks make some solid points, [doomsday murder cults notwithstanding](https://en.wikipedia.org/wiki/Zizians). But there is an underlying idiocy in their worldview that seems like it will inevitably become more problematic as they continue to amass political power and intellectual legitimacy.

A loosely defined internet-born movement centered around communities like *LessWrong*, rationalists emphasize Bayesian reasoning, cognitive bias awareness, and "thinking better" to reach truer beliefs and more effective decisions. They tend to frame problems in terms of probability, expected value, and systematic reasoning, sometimes extending this to everyday life choices. They have spawned the Effective Altruism movement that applies utilitarian logic to charitable giving and life choices. EA argues individuals should maximize the positive impact of their resources (time, money, careers) by focusing on interventions with the highest measurable return, often in global health, poverty reduction, animal welfare, and existential risk reduction. Both are disproportionately common in the SF Bay Area, and often overlap with the startup and AI research spaces.

While the rationalists cloak themselves in the language of intellectualism, philosophy and psychology, their actual discourse is of wildly varying quality. On the positive end, folks like Scott Alexander write a lot of interesting, often well-reasoned essays that try to explain or pull apart ideas. On the other end of the spectrum you have things like ["Roko's Basilisk"](https://en.wikipedia.org/wiki/Roko%27s_basilisk) which is just Pascal's wager repackaged for despicable man-child morons, yet somehow hailed as genius innovation. This would all be tiresome at worst, if it weren't for the fact that these ideologies are increasingly popular amongst the self-styled tech titans of industry. Rationalists are especially vocal, and early to the conversation, about "AI Safety" and as such AI research labs are packed with them. And since the power of the Broligarchs is currently ascendant in American politics, these ideologies are getting more influential by the day.

If you were making a hall of fame for advocates of this ideology it would certainly feature Sam Bankman-Fried. SBF, as he styles it, was a founder of one of the largest crypto exchanges, a crypto-adjacent hedge fund, and is currently a connoisseur of prison food after going down for what can be described as a "Madoff-level Ponzi scheme." In some ways he is an easy target—he got caught with his hand in the cookie jar, and that's not even including the cheap ad hominems about him being an off-putting weirdo. But he also exemplifies the rot at the core of the ideology, not because one dishonest person promulgated it, but because it appears he was essentially a true believer. As far as I can tell he actually thought that he was smarter than everyone else because he knew the word Bayesian, he actually thought that he could steal, bribe and lie and that would make him a good person. A hero even. But this is the fundamental problem with this whole scene.

For a first year philosophy student, utilitarianism is easily sold. It's like the most clearly logical moral philosophy—do good for as many people as possible. Awesome, LFG! But if you take more than the mandatory intro class you start to run into the ugly questions that arise. The [trolley problem](https://en.wikipedia.org/wiki/Trolley_problem) starts to get really uncomfortable when it's not modifying the outcome of a disaster, but instead murdering a man in cold blood to use his organs to save 5 lives. This is a judgment, but I think most reasonable people, at least those raised in cultures similar to mine, get squeamish when proposed to torture or enslave a small number of people for the benefit of others—but utilitarians don't. Rationalists and the EA cult have excused these holes in their ideology by saying "We acknowledge this raises hard questions, that's what makes us such intellectuals," but that's a dodge. One of the darlings of the EA community is the idea that by far the best good you can do per dollar is buying mosquito nets for people in the developing world, and at its core this is a sound argument. But as the ["Motte and Bailey"](https://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/) the rationalists love so much illustrates, this is the rational position that masks the irrational aspects.

Yeah, mosquito nets do a lot of good, almost certainly more good than a lot of the charity people engage in. But EA advocates will often take this to insane extremes: "Yeah I am working for Palantir on their child-killing autonomous drone program, but I make so much money that I can buy enough mosquito nets to save 10x that many kids in Mozambique. I'm the hero here!" It's like they figured out that Kant's [Categorical Imperative](https://en.wikipedia.org/wiki/Categorical_imperative) wrecked their whole worldview, and then took exactly the wrong lesson away. Because child lives are fungible, right? It's like dollars on a balance sheet—you can just move them around until it looks good for the investors. Pretty sure that's part of what landed SBF in jail. He didn't even have the bravery to just be greedy and steal; he thought he was the good guy for doing it. And worse even than that, he seems to have thought he was one of the few with the moral clarity to figure out this was the right thing to do. He a very special boy.

Unfortunately for the rest of us, these very special boys are rapidly gaining influence in The Valley and beyond. Perhaps the most irredeemably infested nook are "AI Safety" research teams. These teams are rife with the same sort of people that would be raring to build the orphan-blood-powered mosquito net factories. They talk, at great length, about the need to "align" AI with human values. But what does that mean? We just need to take our extremely clear, simple and objective set of moral rules that all us humans agree on, plop them into Skynet, and then we are good? Good thing we don't disagree on any of that stuff. Kant would be so pleased to know that not only have we figured out exactly what the universal human values are, but we even outsourced them to machines. These folks will ramble breathlessly about how the "X Risk" of AI is so massive—numbers like 80% chance to wipe out humanity—and then keep working for the companies making it happen. This is the real moral decay that EA creates: it's a total abdication of one's personal responsibility. "Yeah, I stole that money but it was for a good reason" is something that most of us have decided doesn't fly. In a way EA is the closest thing we have in real life to the [Paperclip Maximizer](https://en.wikipedia.org/wiki/Instrumental_convergence) rationalists fear so much. They have decided that you can do some math about number of quality-of-life-adjusted human minutes, and that justifies literally whatever you do. And these are the people tasked with designing the moral compass of our impending AI overlords.

It would all be more horrifying if we couldn't take solace in the fact that these people are not the Cassandras they claim to be. Rather than being cursed to utter horrifying prophecy, they seem condemned to authoritatively spewing bullshit. Eliezer Yudkowsky has correctly predicted roughly 10 of the last 0 AI apocalypses. Still, the fact that these types hold the reins of what is likely to be a world-changing technology is upsetting. From my vantage point, a larger risk of AI is that it permanently entrenches in power a swath of morally misguided losers that think they have it all figured out. At least if I get turned into a paperclip it will be over quick. I am not sure I can stomach the thought of an eternity having to listen to idiots explaining Roko's Basilisk to me.
